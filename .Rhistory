search <- regsubsets(FARES ~., data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
View(airtrain2)
# Question 6.3.c.iii
library(leaps)
library(fastDummies)
search <- regsubsets(FARE ~., data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
sum <- summary(search)
sum$which
sum$rsq
sum$adjr2
sum$cp
#Question 6.3.c.iv
set.seed(7)
tuneGrid <- expand.grid(lambda = 10^seq(4, 0, by = -0.1), alpha = 1)
moddy <- caret::train(FARE ~., data = airtrain2,
method = "glmnet",
family = "gaussian",
trControl = trControl,
tuneGrid = tuneGrid)
#Question 6.3.c.iv
set.seed(7)
tuneGrid <- expand.grid(lambda = 10^seq(4, 0, by = -0.1), alpha = 1)
moddy <- caret::train(FARE ~., data = airtrain2, method = "glmnet",
family = "gaussian", trControl = trControl,
tuneGrid = tuneGrid)
#Question 6.3.c.iv
set.seed(7)
trControl2 <- caret:trainControl(method = "cv", number = 5, allowParallel = T)
#Question 6.3.c.iv
set.seed(7)
trControl2 <- caret::trainControl(method = "cv", number = 5, allowParallel = T)
tuneGrid <- expand.grid(lambda = 10^seq(4, 0, by = -0.1), alpha = 1)
moddy <- caret::train(FARE ~., data = airtrain2, method = "glmnet",
family = "gaussian", trControl = trControl2,
tuneGrid = tuneGrid)
moddy$bestTune
coef(moddy$finalModel, s = moddy$bestTune$lambda)
sum(is.na(airtrain2))
sum(is.na(airhold2))
# Question 6.3.c.v
library(ggplot2)
# stepwise predictive accuracy
rbind(Training = mlba::regressionSummary(predict(air_both_mod, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(air_both_mod, airtrain2),
airtrain2$FARE))
stepgain <- gains(airhold2$FARE, predict(air_both_mod, airhold2))
# Question 6.3.c.v
library(ggplot2)
library(gains)
# stepwise predictive accuracy
rbind(Training = mlba::regressionSummary(predict(air_both_mod, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(air_both_mod, airhold2),
airhold2$FARE))
stepgain <- gains(airhold2$FARE, predict(air_both_mod, airhold2))
df99 <- data.frame(percentile = stepgain$depth,
meanResponse = gain$mean.resp / mean(airhold2$FARE))
# Question 6.3.c.v
library(ggplot2)
library(gains)
# stepwise predictive accuracy
rbind(Training = mlba::regressionSummary(predict(air_both_mod, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(air_both_mod, airhold2),
airhold2$FARE))
stepgain <- gains(airhold2$FARE, predict(air_both_mod, airhold2))
df99 <- data.frame(percentile = stepgain$depth,
meanResponse = stepgain$mean.resp / mean(airhold2$FARE))
g99 <- ggplot(df99, aes(x = percentile, y = meanResponse)) +
geom_bar(stat = "identity") +
labs(x = "Percentile", y = "decile mean / global mean", title = "Decile-Wise Lift Chart")
# Question 6.3.c.v
library(ggplot2)
library(gains)
# stepwise predictive accuracy
rbind(Training = mlba::regressionSummary(predict(air_both_mod, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(air_both_mod, airhold2),
airhold2$FARE))
stepgain <- gains(airhold2$FARE, predict(air_both_mod, airhold2))
df99 <- data.frame(percentile = stepgain$depth,
meanResponse = stepgain$mean.resp / mean(airhold2$FARE))
g99 <- ggplot(df99, aes(x = percentile, y = meanResponse)) +
geom_bar(stat = "identity") +
labs(x = "Percentile", y = "decile mean / global mean", title = "Decile-Wise Lift Chart")
g99
library(ggplot2)
library(gains)
# exhaustive predictive accuracy
rbind(Training = mlba::regressionSummary(predict(search, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(search, airhold2),
airhold2$FARE))
library(ggplot2)
library(gains)
# exhaustive predictive accuracy
#rbind(Training = mlba::regressionSummary(predict(search, airtrain2),
#                                        airtrain2$FARE),
#    Holdout = mlba::regressionSummary(predict(search, airhold2),
#                                     airhold2$FARE))
summary(search)
#stepgain2 <- gains(airhold2$FARE, predict(search, airhold2))
#
#df98 <- data.frame(percentile = stepgain2$depth,
#                  meanResponse = stepgain2$mean.resp / mean(airhold2$FARE))
#g98 <- ggplot(df98, aes(x = percentile, y = meanResponse)) +
# geom_bar(stat = "identity") +
#  labs(x = "Percentile", y = "decile mean / global mean", title = "Decile-Wise # # #Lift Chart")
#g98
library(ggplot2)
library(gains)
# lasso predictive accuracy
rbind(Training = mlba::regressionSummary(predict(moddy, airtrain2),
airtrain2$FARE),
Holdout = mlba::regressionSummary(predict(moddy, airhold2),
airhold2$FARE))
stepgain3 <- gains(airhold2$FARE, predict(moddy, airhold2))
df97 <- data.frame(percentile = stepgain3$depth,
meanResponse = stepgain3$mean.resp / mean(airhold2$FARE))
g97 <- ggplot(df97, aes(x = percentile, y = meanResponse)) +
geom_bar(stat = "identity") +
labs(x = "Percentile", y = "decile mean / global mean", title = "Decile-Wise Lift Chart")
g97
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
View(newdat)
View(search)
search$call
search$ress
search$method
search$which
coef(search)
coef(search, 1)
coef(search, 1:12)
coef(search, 12)
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(coef(search, 12), newdat)
coef(search, 12)
coef(search, 12)
moddy
coef(search, 12)
air_both_mod
coef(search, 12)
summary(air_both_mod)
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(coef(search, 12), newdat)
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(search, newdat)
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
install.packages("smallstuff")
install.packages("smallstuff")
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(moddy, newdat)
# Question 6.3.c.vi
newdat <- data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(0), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(moddy, newdat)
predict(air_both_mod, newdat)
coef(search, 12)
#Question 6.3.c.vii
newdat2 <-  data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(1), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
#Question 6.3.c.vii
newdat2 <-  data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(1), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
View(newdat2)
#Question 6.3.c.vii
newdat2 <-  data.frame(COUPON = c(1.202), NEW = c(3), VACATION = c(0),
SW = c(1), HI = c(4442.141), S_INCOME = c(28760),
E_INCOME = c(27664), S_POP = c(4557004), E_POP = c(3195503),
SLOT = 0, GATE = 0, PAX = 12782, DISTANCE = 1976)
predict(moddy, newdat2)
predict(air_both_mod, newdat2)
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
summary(search66)
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
summary(search66)$which
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
sum2 <-summary(search66)
sum2$which
sum2$adjr2
sum2$cp
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
sum2 <-summary(search66)
sum2$which
sum2$adjr2
sum2$cp
sum2$rsq
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
sum2 <-summary(search66)
sum2$which
sum2$adjr2
sum2$cp
sum2$rsq
coef(sum2, 9)
# Question 6.3.c.ix
library(leaps)
library(fastDummies)
search66 <- regsubsets(FARE ~ VACATION + SW + HI + S_INCOME + E_INCOME + S_POP + E_POP + SLOT + GATE + DISTANCE, data = airtrain2, nbest = 1,
nvmax = ncol(airtrain2), method = "exhaustive" )
sum2 <-summary(search66)
sum2$which
sum2$adjr2
sum2$cp
sum2$rsq
coef(search66, 9)
coef(search66, 9) * newdat
coef(search66, 9)
coef(search66, 9) * c(1, 0, 0, 4442.141, 27664, 4557004, 3195503, 0, 0, 1976)
coef(search66, 9) * c(1, 0, 0, 4442.141, 27664, 4557004, 3195503, 0, 0, 1976)
sum(coef(search66, 9) * c(1, 0, 0, 4442.141, 27664, 4557004, 3195503, 0, 0, 1976))
setwd("C:/Users/benjg/Desktop/STA6714/Term_project_sports_articles_objectivity")
library(readxl)
library(ggplot2)
features2 <- read_excel("features2.xlsx")
table(features2$JJS, features2$Label)
table(features2$NNP)
table(features2$WRB)
table(features2$exclamationmarks, features2$Label)
#table(features2$questionmarks)
#table(features2$semicolon)
table(features2$ellipsis)
table(features2$TOs, features2$Label)
# can remove predictors JJS, NNP, WRB, exclamationmark, TOs, and ellipsis due to sparsity or all zeroes
table(features2$sentence1st)
table(features2$sentencelast)
# can remove these as well
feat3 <- features2[,-c(1, 2, 15, 19, 31, 42, 51, 60 , 61 )]
# removed TextID, URL, JJS, NNP, TOs, WRB, ellipsis, sentence1st, sentencelast
# Let 1 = objective and 0 = subjective
feat3$Label <- ifelse(feat3$Label == "objective", 1, 0)
table(feat3$Label)
View(feat3)
# looking for large correlation values
round(cor(feat3[,c(1,43:53)]), 2)
round(cor(feat3[,c(1:17)]), 2)
round(cor(feat3[,c(1,18:32)]), 2)
round(cor(feat3[,c(1,32:42)]), 2)
library(reshape)
cor.mat2 <- round(cor(feat3[,c(1:13)]), 2)
melted.cor.mat2 <- melt(cor.mat2)
ggplot(melted.cor.mat2, aes(x=X1, y=X2, fill=value)) +
geom_tile() + xlab("") + ylab("") + ggtitle("Heatmap for word and grammar predictors") +
scale_fill_distiller(palette="RdBu", limits=c(-1, 1))
# The following variables are being removed due to high correlation with other predictors
feat4 <- feat3[,-c(2,3,4,6,9,10,14,21,26,31,40,41,37,47,52)]
#round(cor(feat4), 2) > .7
cor.mat3 <- round(cor(feat4[,]), 2)
melted.cor.mat3 <- melt(cor.mat3)
ggplot(melted.cor.mat3, aes(x=X1, y=X2, fill=value)) +
geom_tile() + xlab("") + ylab("") + ggtitle("Heatmap for word and grammar predictors") +
scale_fill_distiller(palette="RdBu", limits=c(-1, 1))
#additional removal due to correlation
feat5 <- feat4[,-c(5,7,9,12,13,17,18,19,20,21,22,29,34,36,37)]
cor.mat4 <- round(cor(feat5[,]), 2)
melted.cor.mat4 <- melt(cor.mat4)
ggplot(melted.cor.mat4, aes(x=X1, y=X2, fill=value)) +
geom_tile() + xlab("") + ylab("") + ggtitle("Heatmap for word and grammar predictors") +
scale_fill_distiller(palette="RdBu", limits=c(-1, 1))
View(feat5)
g <- hist(feat5$Quotes, main = "Histogram of number of quotes used",
xlab = "Quotes", col = "brown")
text(g$mids,g$counts,labels=g$counts, adj=c(0.5, -0.5))
table(feat5$Quotes)
g <- hist(feat5$Quotes, main = "Histogram of number of quotes used",
xlab = "Quotes", col = "brown")
text(g$mids,g$counts,labels=g$counts, adj=c(0.5, -0.5))
table(feat5$Quotes)
g <- hist(feat5$Quotes, main = "Histogram of number of quotes used",
xlab = "Quotes", col = "brown")
text(g$mids,g$counts,labels=g$counts, adj=c(0.5, -0.5))
# full model
set.seed(7)
ind <- sample(1:1000, 700, replace = F)
train.df <- feat5_scal[ind,]
# scaling
feat5_scal <- cbind(feat5[,1], scale(feat5[,c(2:23)]))
# full model
set.seed(7)
ind <- sample(1:1000, 700, replace = F)
train.df <- feat5_scal[ind,]
holdout.df <- feat5_scal[-ind,]
logmod1 <- glm(formula = Label ~., family = binomial(link = "logit"), data = train.df)
summary(logmod1)
# Step-wise or bidirection regression applied to full model
step_mod_both <- MASS::stepAIC(
object = logmod1,
direction = "both"
)
step_mod_both
summary(step_mod_both)
pred <- predict(step_mod_both, holdout.df[,2:23])
prob.predictions <- 1 / (1 + exp(-pred))
# ROC curve
library(ROCR)
predob <- prediction(ifelse(prob.predictions > .5, 1, 0), holdout.df$Label)
perf <- performance(predob, "tpr", "fpr")
perf.df <- data.frame(tpr = perf@x.values[[1]],
fpr = perf@y.values[[1]])
ggplot2::ggplot(perf.df, aes(x = tpr, y = fpr))+
geom_line()+
geom_segment(aes(x=0, y=0, xend=1, yend=1), color = "gray", linetype = "dashed")+
labs(x = "1-Specificity", y = "Sensitivity")
performance(predob, measure = "auc")@y.values[[1]]
coef(step_mod_both)
View(train.df)
# pruned tree
dtree_p <- rpart(Label ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "class",
cp = .00001, minsplit = 5, xval = 5)
library(rpart)
library(rpart.plot)
dtree <- rpart(Label ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "class")
rpart.plot(dtree, extra = 1)
# pruned tree
dtree_p <- rpart(Label ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "class",
cp = .00001, minsplit = 5, xval = 5)
printcp(dtree_p)
# based on the table the tree with 4 splits and had the lowest cross validation error of
# 0.49225 and cp of 0.0129199
# pruned tree with cp with lowest xerror
pruneddt <- prune(dtree_p,
cp = .0129199)
sum(dtree_p$frame$var == "<leaf>")
rpart.plot(pruneddt, extra = 1, fallen.leaves = F)
# prediction and confusion matrix for training
pdt_predict_train <- predict(pruneddt, type = "class")
caret::confusionMatrix(as.factor(pdt_predict_train), as.factor(train.df$Label))
# about .825 accuracy
# prediction and confusion matrix for holdout
pdt_predict_holdout <- predict(pruneddt, holdout.df, type = "class")
caret::confusionMatrix(as.factor(pdt_predict_holdout), as.factor(holdout.df$Label))
# For the holdout data the accuracy is about .78
train2 <- train.df
train2$WPS <- train2$`WP$`
holdout2 <- holdout.df
holdout2$WPS <- holdout2$`WP$`
library(randomForest)
rand_for <- randomForest(Label ~ EX + JJR + PDT + RB + WDT + WP + WPS +
Quotes + questionmarks + past, data = train2, ntree = 500, mtry = 4, nodesize = 5, importance = T )
View(train2)
varImpPlot(rand_for, type = 1)
# confusion matrix
rf.pred <- predict(rand_for, holdout2, type = "class")
caret::confusionMatrix(as.factor(ifelse(rf.pred > .5, 1, 0)), as.factor(holdout2$Label))
# Boosting
library(xgboost)
xbg <- caret::train(Label ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "xgbTree", verbosity = 0)
# Boosting
library(xgboost)
xbg <- caret::train(factor(Label) ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "xgbTree", verbosity = 0)
# boosting matrix
boosting.pred <- predict(xbg, holdout2, type = "class")
# boosting matrix
boosting.pred <- predict(xbg, holdout2)
#caret::confusionMatrix(as.factor(ifelse(rf.pred > .5, 1, 0)), as.factor(holdout2$Label))
# boosting matrix
boosting.pred <- predict(xbg, holdout2)
caret::confusionMatrix(as.factor(boosting.pred), as.factor(holdout2$Label))
# bagging and matrix
install.packages("adabag")
#library(adabag)
#bagged <- caret::training(factor(Label) ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
#   Quotes + questionmarks + past, data = train.df, method = "rf", verbosity = 0)
#bagging.pred <- predict(rand_for, holdout2, type = "class")
#caret::confusionMatrix(as.factor(ifelse(rf.pred > .5, 1, 0)), as.factor(holdout2$Label))
install.packages("adabag")
# bagging and matrix
#install.packages("adabag")
library(adabag)
# bagging and matrix
#install.packages("adabag")
bagged <- caret::training(factor(Label) ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "rf", verbosity = 0)
# bagging and matrix
#install.packages("adabag")
bagged <- caret::train(factor(Label) ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "rf", verbosity = 0)
bagging.pred <- predict(bagged, holdout2)
#caret::confusionMatrix(as.factor(ifelse(rf.pred > .5, 1, 0)), as.factor(holdout2$Label))
# bagging and matrix
#install.packages("adabag")
bagged <- caret::train(factor(Label) ~ EX + JJR + PDT + RB + WDT + WP + `WP$` +
Quotes + questionmarks + past, data = train.df, method = "rf", verbosity = 0)
bagging.pred <- predict(bagged, holdout2)
caret::confusionMatrix(as.factor(bagging.pred), as.factor(holdout2$Label))
# naive bayes
library(tidyverse)
library(caret)
library(e1071)
obj_nb <- naiveBayes(factor(Label) ~ factor(EX) + factor(JJR) + factor(PDT) + factor(RB) + factor(WDT) + factor(WP) + factor(WPS) + factor(Quotes) + factor(questionmarks) + factor(past), data = train2)
obj_nb
holdout2$Label <- factor(holdout2$Label)
holdout2$EX <- factor(holdout2$EX)
holdout2$JJR <- factor(holdout2$JJR)
holdout2$PDT <- factor(holdout2$PDT)
holdout2$RB <- factor(holdout2$RB)
holdout2$WDT <- factor(holdout2$WDT)
holdout2$WP <- factor(holdout2$WP)
holdout2$WPS <- factor(holdout2$WPS)
holdout2$Quotes <- factor(holdout2$Quotes)
holdout2$questionmarks <- factor(holdout2$questionmarks)
holdout2$past <- factor(holdout2$past)
nb_pp <- predict(obj_nb, newdata = holdout2, type = "raw")
nb_pred_class <- predict(obj_nb, newdata = holdout2)
df_nb <- data.frame(actual = holdout2$Label, prediced = nb_pred_class, nb_pp)
View(df_nb)
confusionMatrix(as.factor(nb_pred_class), as.factor(holdout2$Label))
confusionMatrix(as.factor(ifelse(nb_pp > .5, 1, 0)), as.factor(holdout2$Label))
View(nb_pp)
train2 %>% mutate(bin_qm = cut(questionmarks, breaks=3))
train2 %>% mutate(bin_quot = cut(Quotes, breaks=3))
train2 %>% mutate(bin_wps = cut(WPS, breaks=3))
holdout2 %>% mutate(bin_qm = cut(questionmarks, breaks=3))
View(holdout2)
train2 %>% mutate(bin_qm = cut(as.numeric(questionmarks), breaks=3))
train2 %>% mutate(bin_quot = cut(as.numeric(Quotes), breaks=3))
train2 %>% mutate(bin_wps = cut(as.numeric(WPS), breaks=3))
holdout2 %>% mutate(bin_qm = cut(as.numeric(questionmarks), breaks=3))
holdout2 %>% mutate(bin_quot = cut(as.numeric(Quotes), breaks=3))
holdout2 %>% mutate(bin_wps = cut(as.numeric(WPS), breaks=3))
library(tidyverse)
library(caret)
library(e1071)
obj_nb <- naiveBayes(factor(Label) ~ factor(bin_qm) + factor(bin_quot) + factor(bin_wps) , data = train2)
train2$bin_qm <- cut(as.numeric(questionmarks), breaks=3)
train2$bin_qm <- cut(as.numeric(train2$questionmarks), breaks=3)
train2$bin_quot <- cut(as.numeric(train2$Quotes), breaks=3)
train2$bin_wps <- cut(as.numeric(train2$WPS), breaks=3)
holdout2$bin_qm <- cut(as.numeric(holdout2$questionmarks), breaks=3)
holdout2$bin_quot <- cut(as.numeric(holdout2$Quotes), breaks=3)
holdout2$bin_wps <- cut(as.numeric(holdout2$WPS), breaks=3)
library(tidyverse)
library(caret)
library(e1071)
obj_nb <- naiveBayes(factor(Label) ~ factor(bin_qm) + factor(bin_quot) + factor(bin_wps) , data = train2)
nb_pred_class <- predict(obj_nb, newdata = holdout2)
confusionMatrix(as.factor(nb_pred_class), as.factor(holdout2$Label))
train2$bin_qm <- cut(as.numeric(train2$questionmarks), breaks=3)
train2$bin_quot <- cut(as.numeric(train2$Quotes), breaks=3)
train2$bin_wps <- cut(as.numeric(train2$WPS), breaks=3)
holdout2$bin_qm <- cut(as.numeric(holdout2$questionmarks), breaks=3)
holdout2$bin_quot <- cut(as.numeric(holdout2$Quotes), breaks=3)
holdout2$bin_wps <- cut(as.numeric(holdout2$WPS), breaks=3)
library(tidyverse)
library(caret)
library(e1071)
obj_nb <- naiveBayes(factor(Label) ~ factor(bin_qm) + factor(bin_quot) + factor(bin_wps) , data = train2)
obj_nb
nb_pred_class <- predict(obj_nb, newdata = holdout2)
confusionMatrix(as.factor(nb_pred_class), as.factor(holdout2$Label))
